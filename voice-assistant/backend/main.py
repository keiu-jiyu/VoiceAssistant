import os
import asyncio
from dotenv import load_dotenv
import logging
import sys
import aiohttp
import uvicorn

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from livekit import api, rtc
from livekit.agents.llm import ChatContext
from livekit.agents.stt import SpeechEventType
from livekit.plugins import aliyun
from custom_aliyun_stt import AliyunSTT
# --- é…ç½® ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
load_dotenv()

# --- LiveKit é…ç½® ---
LIVEKIT_URL = os.environ.get("LIVEKIT_URL")
LIVEKIT_API_KEY = os.environ.get("LIVEKIT_API_KEY")
LIVEKIT_API_SECRET = os.environ.get("LIVEKIT_API_SECRET")
ROOM_NAME = "my-voice-room"
AGENT_IDENTITY = "ai-assistant"

# --- é˜¿é‡Œäº‘é…ç½® ---
DASHSCOPE_API_KEY = os.environ.get("DASHSCOPE_API_KEY")

# --- æ£€æŸ¥ç¯å¢ƒå˜é‡ ---
required_vars = [
    "LIVEKIT_URL", "LIVEKIT_API_KEY", "LIVEKIT_API_SECRET", "DASHSCOPE_API_KEY"
]
for var in required_vars:
    if not os.environ.get(var):
        raise ValueError(f"ç¯å¢ƒå˜é‡ '{var}' æœªè®¾ç½®")

# --- å…¨å±€å˜é‡ ---
active_rooms = {}


# --- AI åŠ©æ‰‹æ ¸å¿ƒé€»è¾‘ ---
class AIAssistant:
    def __init__(self):
        self.room = None
        self.audio_source = None
        self.stt = None
        self.llm = None
        self.tts = None
        self.http_session = None

    async def initialize(self):
        """åˆå§‹åŒ–AIç»„ä»¶"""
        self.http_session = aiohttp.ClientSession()
        self.stt = AliyunSTT(
            api_key=DASHSCOPE_API_KEY,
            model='paraformer-realtime-v2',
            language='zh-CN'
        )
        self.llm = aliyun.LLM(model="qwen3-max", api_key=DASHSCOPE_API_KEY)
        self.tts = aliyun.TTS(
            model='cosyvoice-v1',  # â† ä½¿ç”¨å®˜æ–¹æ¨èæ¨¡å‹
            voice='longxiaochun',  # å¯é€‰ï¼šéŸ³è‰²
            http_session=self.http_session
        )

    async def connect_to_room(self):
        """è¿æ¥åˆ°LiveKitæˆ¿é—´"""
        token = (
            api.AccessToken(LIVEKIT_API_KEY, LIVEKIT_API_SECRET)
            .with_identity(AGENT_IDENTITY)
            .with_name("AI Assistant")
            .with_grants(api.VideoGrants(
                room_join=True,
                room=ROOM_NAME,
                can_publish=True,
                can_publish_data=True,
                agent=True,
            ))
        ).to_jwt()

        self.room = rtc.Room()
        await self.room.connect(LIVEKIT_URL, token)
        logging.info(f"âœ… AI å·²è¿æ¥åˆ°æˆ¿é—´: {ROOM_NAME}")

        # åˆ›å»ºå¹¶å‘å¸ƒéŸ³é¢‘è½¨é“
        self.audio_source = rtc.AudioSource(
            self.tts.sample_rate,
            self.tts.num_channels
        )
        track = rtc.LocalAudioTrack.create_audio_track(
            "ai-voice",
            self.audio_source
        )
        publication = await self.room.local_participant.publish_track(track)
        logging.info(f"ğŸ¤ AI è¯­éŸ³è½¨é“å·²å‘å¸ƒ: {publication.sid}")

    async def process_participant_audio(self, participant: rtc.Participant):
        """å¤„ç†å‚ä¸è€…çš„éŸ³é¢‘æµ"""
        if participant.identity == AGENT_IDENTITY:
            return

        # æŸ¥æ‰¾éŸ³é¢‘è½¨é“
        audio_stream = None
        for pub in participant.track_publications.values():
            if (pub.track and
                    pub.kind == rtc.TrackKind.KIND_AUDIO and
                    pub.source == rtc.TrackSource.SOURCE_MICROPHONE):
                audio_stream = rtc.AudioStream(pub.track)
                break

        if not audio_stream:
            logging.warning(f"æœªæ‰¾åˆ°ç”¨æˆ· {participant.identity} çš„éº¦å…‹é£è½¨é“")
            return

        logging.info(f"ğŸ§ å¼€å§‹ç›‘å¬ç”¨æˆ·: {participant.identity}")
        stt_stream = self.stt.stream()
        logging.info("âœ… STT æµå·²åˆ›å»º")
        chat_context = ChatContext()

        async def feed_stt(audio_stream, stt_stream):
            """ä»éŸ³é¢‘æµè¯»å–æ•°æ®å¹¶å‘é€åˆ° STT"""
            try:
                # åˆ›å»ºé‡é‡‡æ ·å™¨ï¼š48000Hz â†’ 16000Hz
                resampler = rtc.AudioResampler(
                    input_rate=48000,
                    output_rate=16000,
                    num_channels=1,
                    quality=rtc.AudioResamplerQuality.QUICK
                )

                frame_count = 0
                resampled_count = 0

                logging.info("ğŸ”„ å¼€å§‹éŸ³é¢‘é‡é‡‡æ ·æµç¨‹")

                async for frame_event in audio_stream:
                    frame = frame_event.frame
                    frame_count += 1

                    if frame_count == 1:
                        logging.info(
                            f"ğŸµ é¦–å¸§éŸ³é¢‘: sample_rate={frame.sample_rate}, channels={frame.num_channels}, samples={frame.samples_per_channel}")

                    # æ‰“å°åŸå§‹éŸ³é¢‘æ•°æ®
                    if frame_count % 100 == 0:
                        logging.info(f"ğŸ“¡ å·²æ¥æ”¶ {frame_count} ä¸ªåŸå§‹å¸§ (48000Hz)")

                    # æ¨é€åŸå§‹å¸§åˆ°é‡é‡‡æ ·å™¨ï¼Œå¹¶ç«‹å³å¤„ç†è¾“å‡º
                    for resampled_frame in resampler.push(frame):
                        resampled_count += 1

                        # æ‰“å°é‡é‡‡æ ·åçš„æ•°æ®
                        #if resampled_count % 10 == 0:
                            #logging.info(f"ğŸ”„ å·²ç”Ÿæˆ {resampled_count} ä¸ªé‡é‡‡æ ·å¸§ (16000Hz)")

                        # æ¨é€åˆ° STT
                        stt_stream.push_frame(resampled_frame)

                        if resampled_count % 50 == 0:
                            logging.info(f"âœ… å·²æ¨é€ {resampled_count} ä¸ªå¸§åˆ° STT")

                # ğŸ”¥ é‡è¦ï¼šåˆ·æ–°é‡é‡‡æ ·å™¨ï¼Œè·å–å‰©ä½™æ•°æ®
                logging.info("ğŸ”„ åˆ·æ–°é‡é‡‡æ ·å™¨ï¼Œè·å–å‰©ä½™éŸ³é¢‘...")
                for resampled_frame in resampler.flush():
                    resampled_count += 1
                    stt_stream.push_frame(resampled_frame)

                logging.info(f"âœ… éŸ³é¢‘æµå¤„ç†å®Œæˆ: åŸå§‹å¸§={frame_count}, é‡é‡‡æ ·å¸§={resampled_count}")

            except Exception as e:
                logging.error(f"âŒ éŸ³é¢‘æµå¤„ç†é”™è¯¯: {e}")
            finally:
                await stt_stream.aclose()

        async def handle_stt():
            """å¤„ç†STTç»“æœå¹¶ç”Ÿæˆå›å¤"""
            logging.info("ğŸ¯ STT äº‹ä»¶ç›‘å¬å™¨å·²å¯åŠ¨")
            try:
                async for event in stt_stream:
                    logging.info(f"ğŸ“ STTäº‹ä»¶: type={event.type}")
                    if event.alternatives:
                        logging.info(f"   æ–‡æœ¬: '{event.alternatives[0].text}'")

                    if (event.type == SpeechEventType.FINAL_TRANSCRIPT and
                            event.alternatives):
                        user_text = event.alternatives[0].text.strip()
                        if not user_text:
                            logging.warning("è¯†åˆ«ç»“æœä¸ºç©ºï¼Œè·³è¿‡")
                            continue

                        logging.info(f"ğŸ’¬ ç”¨æˆ· ({participant.identity}) è¯´: '{user_text}'")
                        chat_context.add_message(role="user", content=user_text)

                        logging.info("ğŸ§  AI æ­£åœ¨æ€è€ƒ...")
                        full_response = ""

                        try:
                            # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ç‹¬ç«‹ä»»åŠ¡å¤„ç† TTSï¼Œé¿å…é˜»å¡ STT
                            async def process_llm_and_tts():
                                nonlocal full_response
                                tts_stream = self.tts.stream()

                                llm_stream = self.llm.chat(chat_ctx=chat_context)
                                async for chunk in llm_stream:
                                    if chunk.delta and chunk.delta.content:
                                        content = chunk.delta.content
                                        tts_stream.push_text(content)
                                        full_response += content

                                tts_stream.flush()
                                chat_context.add_message(role="assistant", content=full_response)
                                logging.info(f"ğŸ¤– AI å›ç­”: '{full_response}'")

                                # æ’­æ”¾ TTS éŸ³é¢‘
                                logging.info("ğŸ”Š å¼€å§‹æ’­æ”¾ TTS éŸ³é¢‘")
                                frame_count = 0
                                async for audio_chunk in tts_stream:
                                    try:
                                        if hasattr(audio_chunk, 'frame'):
                                            await self.audio_source.capture_frame(audio_chunk.frame)
                                            frame_count += 1
                                        elif audio_chunk:
                                            await self.audio_source.capture_frame(audio_chunk)
                                            frame_count += 1
                                    except Exception as e:
                                        logging.error(f"æ’­æ”¾éŸ³é¢‘å¸§å¤±è´¥: {e}")

                                logging.info(f"âœ… TTS éŸ³é¢‘æ’­æ”¾å®Œæˆï¼Œå…±æ’­æ”¾ {frame_count} å¸§")

                                # ğŸ”§ å…³é”®ï¼šç¡®ä¿ TTS æµæ­£ç¡®å…³é—­
                                await tts_stream.aclose()

                            # åˆ›å»ºç‹¬ç«‹ä»»åŠ¡ï¼Œé¿å…é˜»å¡ STT äº‹ä»¶å¾ªç¯
                            asyncio.create_task(process_llm_and_tts())

                        except Exception as e:
                            logging.error(f"LLM/TTS å¤„ç†é”™è¯¯: {e}", exc_info=True)

            except Exception as e:
                logging.error(f"STT å¤„ç†é”™è¯¯: {e}", exc_info=True)

        # å¹¶è¡Œå¤„ç†éŸ³é¢‘æµå’ŒSTT
        await asyncio.gather(
            feed_stt(audio_stream, stt_stream),
            handle_stt(),
            return_exceptions=True
        )

    async def start(self):
        """å¯åŠ¨AIåŠ©æ‰‹"""
        try:
            await self.initialize()
            await self.connect_to_room()

            # ç›‘å¬è½¨é“è®¢é˜…äº‹ä»¶ï¼ˆè¿™æ˜¯å…³é”®ï¼ï¼‰
            @self.room.on("track_subscribed")
            def on_track_subscribed(
                    track: rtc.Track,
                    publication: rtc.RemoteTrackPublication,
                    participant: rtc.RemoteParticipant
            ):
                logging.info(
                    f"ğŸ”” è®¢é˜…è½¨é“: {participant.identity} - Source:{publication.source} Kind:{publication.kind}")

                # åªåœ¨è¿™é‡Œå¤„ç†éº¦å…‹é£éŸ³é¢‘
                if (publication.kind == rtc.TrackKind.KIND_AUDIO and
                        publication.source == rtc.TrackSource.SOURCE_MICROPHONE):
                    logging.info(f"ğŸ¤ æ£€æµ‹åˆ°éº¦å…‹é£è½¨é“ï¼Œå¼€å§‹å¤„ç†ç”¨æˆ·: {participant.identity}")
                    asyncio.create_task(self.process_participant_audio(participant))

            # ç›‘å¬æ–°å‚ä¸è€…ï¼ˆä»…ç”¨äºæ—¥å¿—ï¼‰
            @self.room.on("participant_connected")
            def on_participant_connected(participant: rtc.RemoteParticipant):
                logging.info(f"ğŸ‘¤ æ–°ç”¨æˆ·åŠ å…¥: {participant.identity}")

            for participant in self.room.remote_participants.values():
                for pub in participant.track_publications.values():
                    if (pub.kind == rtc.TrackKind.KIND_AUDIO and
                            pub.source == rtc.TrackSource.SOURCE_MICROPHONE):
                                pub.set_subscribed(True)  # â† æ”¹æˆ 12 ä¸ªç©ºæ ¼ï¼ˆ3å±‚ç¼©è¿› Ã— 4ç©ºæ ¼ï¼‰

            logging.info("âœ¨ AI Agent å‡†å¤‡å°±ç»ª")
            await asyncio.Event().wait()

        except Exception as e:
            logging.error(f"AIåŠ©æ‰‹è¿è¡Œé”™è¯¯: {e}")
        finally:
            await self.cleanup()

    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.http_session:
            await self.http_session.close()
        if self.room:
            await self.room.disconnect()
        logging.info("ğŸšª AI åŠ©æ‰‹å·²å…³é—­")


async def ai_assistant_task():
    """å¯åŠ¨AIåŠ©æ‰‹ä»»åŠ¡"""
    assistant = AIAssistant()
    await assistant.start()


# --- FastAPI Web æœåŠ¡å™¨ ---
app = FastAPI(title="è¯­éŸ³åŠ©æ‰‹API")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"]
)


@app.get("/")
async def root():
    return {"message": "è¯­éŸ³åŠ©æ‰‹APIæœåŠ¡è¿è¡Œä¸­"}


@app.get("/token")  # â† æ”¹è¿™é‡Œï¼špost â†’ getï¼Œè·¯å¾„æ”¹ä¸º /token
async def create_token_endpoint():
    """ä¸ºç”¨æˆ·ç”ŸæˆåŠ å…¥æˆ¿é—´çš„token"""
    identity = f"user-{int(asyncio.get_event_loop().time())}"
    token = (
        api.AccessToken(LIVEKIT_API_KEY, LIVEKIT_API_SECRET)
        .with_identity(identity)
        .with_name(identity)
        .with_grants(api.VideoGrants(
            room_join=True,
            room=ROOM_NAME,
            can_publish=True,
            can_subscribe=True
        ))
    ).to_jwt()
    return {"token": token, "identity": identity}


@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return {"status": "healthy", "service": "voice-assistant"}


# --- ä¸»ç¨‹åºå…¥å£ ---
if __name__ == "__main__":
    if len(sys.argv) > 1 and sys.argv[1] == 'agent':
        logging.info("å¯åŠ¨æ¨¡å¼: AI Agent")
        try:
            asyncio.run(ai_assistant_task())
        except KeyboardInterrupt:
            logging.info("AI Agent è¢«æ‰‹åŠ¨åœæ­¢")
    else:
        logging.info("å¯åŠ¨æ¨¡å¼: FastAPI Server")
        uvicorn.run(
            app,
            host="0.0.0.0",
            port=8000,
            log_level="info"
        )